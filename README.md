Next-Word Predictor Using LSTM in PyTorch

A powerful Next-Word Prediction model built from scratch using PyTorch and LSTM (Long Short-Term Memory) recurrent neural networks. This project demonstrates how to preprocess text data, train a sequence model on a GPU, and achieve high predictive accuracy â€” a great foundation for understanding language modeling and how modern LLMs work.

ğŸš€ Project Overview

Natural language tasks like predicting the next word in a sentence are key building blocks for text generation, autocompletion, and even large language models. This repository shows how an LSTM-based model can be trained to learn patterns in text sequences and predict the most likely next word based on previous context.

In this project, we:

âœ… Converted raw text into trainable sequences
âœ… Built a PyTorch LSTM neural network
âœ… Trained the model on GPU for faster performance
âœ… Achieved ~98% accuracy on the prediction task
âœ… Explored inference and next-word generation

ğŸ§  Why LSTM?

LSTM networks are a type of Recurrent Neural Network (RNN) that can capture long-term dependencies in sequential data. They overcome many limitations of basic RNNs and are widely used in language modeling tasks like next-word prediction, machine translation, and text generation.

ğŸ“¦ Features

ğŸ§  PyTorch implementation â€” clean, readable, and beginner-friendly

âš¡ GPU training support â€” takes advantage of CUDA for fast learning

ğŸ“ˆ High accuracy â€” impressive performance on next-word prediction

ğŸ“ Jupyter Notebook included â€” walk through all steps interactively

ğŸ Complete training + inference pipeline

ğŸ› ï¸ How It Works

Text Preprocessing

Tokenize sentences

Turn words into integer sequences

Create input/output pairs for supervision

Model Architecture

Embedding layer

LSTM layers

Fully connected output layer

Softmax for word prediction

Training

Trained on text data using GPU (if available)

Loss & accuracy metrics logged

Inference

Feed a sequence of words

Model predicts the most probable next word
